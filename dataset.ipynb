{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://datasets-server.huggingface.co/parquet?dataset=ibm/duorc\"\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "data = query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eb30d441c9405d8690b8875a05f019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/491k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80c73c5f9e8402881c43f2d68054236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'drug', 'effect', 'indexes'],\n",
       "        num_rows: 6821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "d = load_dataset('ade_corpus_v2', 'Ade_corpus_v2_drug_ade_relation')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intravenous azithromycin-induced ototoxicity. azithromycin ototoxicity {'drug': {'start_char': [12], 'end_char': [24]}, 'effect': {'start_char': [33], 'end_char': [44]}}\n",
      "Immobilization, while Paget's bone disease was present, and perhaps enhanced activation of dihydrotachysterol by rifampicin, could have led to increased calcium-release into the circulation. dihydrotachysterol increased calcium-release {'drug': {'start_char': [91], 'end_char': [109]}, 'effect': {'start_char': [143], 'end_char': [168]}}\n",
      "Unaccountable severe hypercalcemia in a patient treated for hypoparathyroidism with dihydrotachysterol. dihydrotachysterol hypercalcemia {'drug': {'start_char': [84], 'end_char': [102]}, 'effect': {'start_char': [21], 'end_char': [34]}}\n",
      "METHODS: We report two cases of pseudoporphyria caused by naproxen and oxaprozin. naproxen pseudoporphyria {'drug': {'start_char': [58], 'end_char': [66]}, 'effect': {'start_char': [32], 'end_char': [47]}}\n",
      "METHODS: We report two cases of pseudoporphyria caused by naproxen and oxaprozin. oxaprozin pseudoporphyria {'drug': {'start_char': [71], 'end_char': [80]}, 'effect': {'start_char': [32], 'end_char': [47]}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(d['train']['text'][i], d['train']['drug'][i], d['train']['effect'][i], d['train']['indexes'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-RMKATsQOxWHQxesW4rQyT3BlbkFJq6pK9cWm19lUysCrG6QW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompose a poem that explains the concept of recursion in programming.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:1213\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1201\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1210\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1211\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:902\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glkb/lib/python3.10/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    992\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    996\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    997\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1001\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='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')\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oxaprozin'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dataset in module datasets.arrow_dataset object:\n",
      "\n",
      "class Dataset(DatasetInfoMixin, datasets.search.IndexableMixin, TensorflowDatasetMixin)\n",
      " |  Dataset(arrow_table: datasets.table.Table, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_table: Optional[datasets.table.Table] = None, fingerprint: Optional[str] = None)\n",
      " |  \n",
      " |  A Dataset backed by an Arrow table.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      DatasetInfoMixin\n",
      " |      datasets.search.IndexableMixin\n",
      " |      TensorflowDatasetMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\n",
      " |  \n",
      " |  __getitems__(self, keys: List) -> List\n",
      " |      Can be used to get a batch using a list of integers indices.\n",
      " |  \n",
      " |  __init__(self, arrow_table: datasets.table.Table, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_table: Optional[datasets.table.Table] = None, fingerprint: Optional[str] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate through the examples.\n",
      " |      \n",
      " |      If a formatting is set with :meth:`Dataset.set_format` rows will be returned with the\n",
      " |      selected format.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Number of rows in the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.__len__\n",
      " |      <bound method Dataset.__len__ of Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 1066\n",
      " |      })>\n",
      " |      ```\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_column(self, name: str, column: Union[list, <built-in function array>], new_fingerprint: str)\n",
      " |      Add column to Dataset.\n",
      " |      \n",
      " |      <Added version=\"1.7\"/>\n",
      " |      \n",
      " |      Args:\n",
      " |          name (`str`):\n",
      " |              Column name.\n",
      " |          column (`list` or `np.array`):\n",
      " |              Column data to be added.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> more_text = ds[\"text\"]\n",
      " |      >>> ds.add_column(name=\"text_2\", column=more_text)\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label', 'text_2'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  add_elasticsearch_index(self, column: str, index_name: Optional[str] = None, host: Optional[str] = None, port: Optional[int] = None, es_client: Optional[ForwardRef('elasticsearch.Elasticsearch')] = None, es_index_name: Optional[str] = None, es_index_config: Optional[dict] = None)\n",
      " |      Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              The column of the documents to add to the index.\n",
      " |          index_name (`str`, *optional*):\n",
      " |              The `index_name`/identifier of the index.\n",
      " |              This is the index name that is used to call [`~Dataset.get_nearest_examples`] or [`Dataset.search`].\n",
      " |              By default it corresponds to `column`.\n",
      " |          host (`str`, *optional*, defaults to `localhost`):\n",
      " |              Host of where ElasticSearch is running.\n",
      " |          port (`str`, *optional*, defaults to `9200`):\n",
      " |              Port of where ElasticSearch is running.\n",
      " |          es_client (`elasticsearch.Elasticsearch`, *optional*):\n",
      " |              The elasticsearch client used to create the index if host and port are `None`.\n",
      " |          es_index_name (`str`, *optional*):\n",
      " |              The elasticsearch index name used to create the index.\n",
      " |          es_index_config (`dict`, *optional*):\n",
      " |              The configuration of the elasticsearch index.\n",
      " |              Default config is:\n",
      " |                  ```\n",
      " |                  {\n",
      " |                      \"settings\": {\n",
      " |                          \"number_of_shards\": 1,\n",
      " |                          \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
      " |                      },\n",
      " |                      \"mappings\": {\n",
      " |                          \"properties\": {\n",
      " |                              \"text\": {\n",
      " |                                  \"type\": \"text\",\n",
      " |                                  \"analyzer\": \"standard\",\n",
      " |                                  \"similarity\": \"BM25\"\n",
      " |                              },\n",
      " |                          }\n",
      " |                      },\n",
      " |                  }\n",
      " |                  ```\n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> es_client = elasticsearch.Elasticsearch()\n",
      " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |      >>> ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name=\"my_es_index\")\n",
      " |      >>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n",
      " |      ```\n",
      " |  \n",
      " |  add_faiss_index(self, column: str, index_name: Optional[str] = None, device: Optional[int] = None, string_factory: Optional[str] = None, metric_type: Optional[int] = None, custom_index: Optional[ForwardRef('faiss.Index')] = None, batch_size: int = 1000, train_size: Optional[int] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
      " |      Add a dense index using Faiss for fast retrieval.\n",
      " |      By default the index is done over the vectors of the specified column.\n",
      " |      You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n",
      " |      You can find more information about Faiss here:\n",
      " |      \n",
      " |      - For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              The column of the vectors to add to the index.\n",
      " |          index_name (`str`, *optional*):\n",
      " |              The `index_name`/identifier of the index.\n",
      " |              This is the `index_name` that is used to call [`~datasets.Dataset.get_nearest_examples`] or [`~datasets.Dataset.search`].\n",
      " |              By default it corresponds to `column`.\n",
      " |          device (`Union[int, List[int]]`, *optional*):\n",
      " |              If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
      " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      " |          string_factory (`str`, *optional*):\n",
      " |              This is passed to the index factory of Faiss to create the index.\n",
      " |              Default index class is `IndexFlat`.\n",
      " |          metric_type (`int`, *optional*):\n",
      " |              Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n",
      " |          custom_index (`faiss.Index`, *optional*):\n",
      " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
      " |          batch_size (`int`):\n",
      " |              Size of the batch to use while adding vectors to the `FaissIndex`. Default value is `1000`.\n",
      " |              <Added version=\"2.4.0\"/>\n",
      " |          train_size (`int`, *optional*):\n",
      " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      " |          faiss_verbose (`bool`, defaults to `False`):\n",
      " |              Enable the verbosity of the Faiss index.\n",
      " |          dtype (`data-type`):\n",
      " |              The dtype of the numpy arrays that are indexed.\n",
      " |              Default is `np.float32`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |      >>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n",
      " |      >>> ds_with_embeddings.add_faiss_index(column='embeddings')\n",
      " |      >>> # query\n",
      " |      >>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      " |      >>> # save index\n",
      " |      >>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
      " |      \n",
      " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |      >>> # load index\n",
      " |      >>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
      " |      >>> # query\n",
      " |      >>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      " |      ```\n",
      " |  \n",
      " |  add_faiss_index_from_external_arrays(self, external_arrays: <built-in function array>, index_name: str, device: Optional[int] = None, string_factory: Optional[str] = None, metric_type: Optional[int] = None, custom_index: Optional[ForwardRef('faiss.Index')] = None, batch_size: int = 1000, train_size: Optional[int] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
      " |      Add a dense index using Faiss for fast retrieval.\n",
      " |      The index is created using the vectors of `external_arrays`.\n",
      " |      You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n",
      " |      You can find more information about Faiss here:\n",
      " |      \n",
      " |      - For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n",
      " |      \n",
      " |      Args:\n",
      " |          external_arrays (`np.array`):\n",
      " |              If you want to use arrays from outside the lib for the index, you can set `external_arrays`.\n",
      " |              It will use `external_arrays` to create the Faiss index instead of the arrays in the given `column`.\n",
      " |          index_name (`str`):\n",
      " |              The `index_name`/identifier of the index.\n",
      " |              This is the `index_name` that is used to call [`~datasets.Dataset.get_nearest_examples`] or [`~datasets.Dataset.search`].\n",
      " |          device (Optional `Union[int, List[int]]`, *optional*):\n",
      " |              If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
      " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      " |          string_factory (`str`, *optional*):\n",
      " |              This is passed to the index factory of Faiss to create the index.\n",
      " |              Default index class is `IndexFlat`.\n",
      " |          metric_type (`int`, *optional*):\n",
      " |              Type of metric. Ex: `faiss.faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n",
      " |          custom_index (`faiss.Index`, *optional*):\n",
      " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n",
      " |              <Added version=\"2.4.0\"/>\n",
      " |          train_size (`int`, *optional*):\n",
      " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      " |          faiss_verbose (`bool`, defaults to False):\n",
      " |              Enable the verbosity of the Faiss index.\n",
      " |          dtype (`numpy.dtype`):\n",
      " |              The dtype of the numpy arrays that are indexed. Default is np.float32.\n",
      " |  \n",
      " |  add_item(self, item: dict, new_fingerprint: str)\n",
      " |      Add item to Dataset.\n",
      " |      \n",
      " |      <Added version=\"1.7\"/>\n",
      " |      \n",
      " |      Args:\n",
      " |          item (`dict`):\n",
      " |              Item data to be added.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
      " |      >>> ds = ds.add_item(new_review)\n",
      " |      >>> ds[-1]\n",
      " |      {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
      " |      ```\n",
      " |  \n",
      " |  align_labels_with_mapping(self, label2id: Dict, label_column: str) -> 'Dataset'\n",
      " |      Align the dataset's label ID and label name mapping to match an input `label2id` mapping.\n",
      " |      This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n",
      " |      The alignment in done using the lowercase label names.\n",
      " |      \n",
      " |      Args:\n",
      " |          label2id (`dict`):\n",
      " |              The label name to ID mapping to align the dataset with.\n",
      " |          label_column (`str`):\n",
      " |              The column name of labels to align on.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
      " |      >>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
      " |      >>> # mapping to align with\n",
      " |      >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
      " |      >>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
      " |      ```\n",
      " |  \n",
      " |  cast(self, features: datasets.features.features.Features, batch_size: Optional[int] = 1000, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, num_proc: Optional[int] = None) -> 'Dataset'\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      \n",
      " |      Args:\n",
      " |          features ([`Features`]):\n",
      " |              New features to cast the dataset to.\n",
      " |              The name of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. `str` <-> `ClassLabel` you should use [`~datasets.Dataset.map`] to update the Dataset.\n",
      " |          batch_size (`int`, defaults to `1000`):\n",
      " |              Number of examples per batch provided to cast.\n",
      " |              If `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to cast.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          load_from_cache_file (`bool`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (`str`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running [`~datasets.Dataset.map`].\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]: A copy of the dataset with casted features.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset, ClassLabel, Value\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      >>> new_features = ds.features.copy()\n",
      " |      >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      " |      >>> new_features['text'] = Value('large_string')\n",
      " |      >>> ds = ds.cast(new_features)\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " |       'text': Value(dtype='large_string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  cast_column(self, column: str, feature: Union[dict, list, tuple, datasets.features.features.Value, datasets.features.features.ClassLabel, datasets.features.translation.Translation, datasets.features.translation.TranslationVariableLanguages, datasets.features.features.Sequence, datasets.features.features.Array2D, datasets.features.features.Array3D, datasets.features.features.Array4D, datasets.features.features.Array5D, datasets.features.audio.Audio, datasets.features.image.Image], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Cast column to feature for decoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              Column name.\n",
      " |          feature (`FeatureType`):\n",
      " |              Target feature.\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  class_encode_column(self, column: str, include_nulls: bool = False) -> 'Dataset'\n",
      " |      Casts the given column as [`~datasets.features.ClassLabel`] and updates the table.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              The name of the column to cast (list all the column names with [`~datasets.Dataset.column_names`])\n",
      " |          include_nulls (`bool`, defaults to `False`):\n",
      " |              Whether to include null values in the class labels. If `True`, the null values will be encoded as the `\"None\"` class label.\n",
      " |      \n",
      " |              <Added version=\"1.14.2\"/>\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"boolq\", split=\"validation\")\n",
      " |      >>> ds.features\n",
      " |      {'answer': Value(dtype='bool', id=None),\n",
      " |       'passage': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None)}\n",
      " |      >>> ds = ds.class_encode_column('answer')\n",
      " |      >>> ds.features\n",
      " |      {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
      " |       'passage': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  cleanup_cache_files(self) -> int\n",
      " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is\n",
      " |      one.\n",
      " |      \n",
      " |      Be careful when running this command that no other process is currently using other cache files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of removed files.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.cleanup_cache_files()\n",
      " |      10\n",
      " |      ```\n",
      " |  \n",
      " |  export(self, filename: str, format: str = 'tfrecord')\n",
      " |      Writes the Arrow dataset to a TFRecord file.\n",
      " |      \n",
      " |      The dataset must already be in tensorflow format. The records will be written with\n",
      " |      keys from `dataset._format_columns`.\n",
      " |      \n",
      " |      Args:\n",
      " |          filename (`str`): The filename, including the `.tfrecord` extension, to write to.\n",
      " |          format (`str`, optional, default `\"tfrecord\"`): The type of output file. Currently this is a no-op, as\n",
      " |              TFRecords are the only option. This enables a more flexible function signature later.\n",
      " |  \n",
      " |  filter(self, function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`Callable`): Callable with one of the following signatures:\n",
      " |      \n",
      " |              - `function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |              - `function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |      \n",
      " |              If no function is provided, defaults to an always `True` function: `lambda x: True`.\n",
      " |          with_indices (`bool`, defaults to `False`):\n",
      " |              Provide example indices to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
      " |          with_rank (`bool`, defaults to `False`):\n",
      " |              Provide process rank to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      " |          input_columns (`str` or `List[str]`, *optional*):\n",
      " |              The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`):\n",
      " |              Provide batch of examples to `function`.\n",
      " |          batch_size (`int`, *optional*, defaults to `1000`):\n",
      " |              Number of examples per batch provided to `function` if\n",
      " |              `batched = True`. If `batched = False`, one example per batch is passed to `function`.\n",
      " |              If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (`str`, *optional*):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          fn_kwargs (`dict`, *optional*):\n",
      " |              Keyword arguments to be passed to `function`.\n",
      " |          num_proc (`int`, *optional*):\n",
      " |              Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |          suffix_template (`str`):\n",
      " |              If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n",
      " |              For example, if `cache_file_name` is `\"processed.arrow\"`, then for `rank = 1` and `num_proc = 4`,\n",
      " |              the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix (default\n",
      " |              `_{rank:05d}_of_{num_proc:05d}`).\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |          desc (`str`, *optional*, defaults to `None`):\n",
      " |              Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 533\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  flatten(self, new_fingerprint: Optional[str] = None, max_depth=16) -> 'Dataset'\n",
      " |      Flatten the table.\n",
      " |      Each column with a struct type is flattened into one column per struct field.\n",
      " |      Other columns are left unchanged.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]: A copy of the dataset with flattened columns.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"squad\", split=\"train\")\n",
      " |      >>> ds.features\n",
      " |      {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " |       'context': Value(dtype='string', id=None),\n",
      " |       'id': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None),\n",
      " |       'title': Value(dtype='string', id=None)}\n",
      " |      >>> ds.flatten()\n",
      " |      Dataset({\n",
      " |          features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " |          num_rows: 87599\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  flatten_indices(self, keep_in_memory: bool = False, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, num_proc: Optional[int] = None, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create and cache a new Dataset by flattening the indices mapping.\n",
      " |      \n",
      " |      Args:\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          cache_file_name (`str`, *optional*, default `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          features (`Optional[datasets.Features]`, defaults to `None`):\n",
      " |              Use a specific [`Features`] to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `False`):\n",
      " |              Allow null values in the table.\n",
      " |          num_proc (`int`, optional, default `None`):\n",
      " |              Max number of processes when generating cache. Already cached shards are loaded sequentially\n",
      " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  formatted_as(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      To be used in a `with` statement. Set `__getitem__` return format (type and columns).\n",
      " |      \n",
      " |      Args:\n",
      " |          type (`str`, *optional*):\n",
      " |              Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n",
      " |              `None` means `__getitem__`` returns python objects (default).\n",
      " |          columns (`List[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              `None` means `__getitem__` returns all columns (default).\n",
      " |          output_all_columns (`bool`, defaults to `False`):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |          **format_kwargs (additional keyword arguments):\n",
      " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  iter(self, batch_size: int, drop_last_batch: bool = False)\n",
      " |      Iterate through the batches of size `batch_size`.\n",
      " |      \n",
      " |      If a formatting is set with [`~datasets.Dataset.set_format`] rows will be returned with the\n",
      " |      selected format.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size (:obj:`int`): size of each batch to yield.\n",
      " |          drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n",
      " |              dropped\n",
      " |  \n",
      " |  map(self, function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, drop_last_batch: bool = False, remove_columns: Union[str, List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset'\n",
      " |      Apply a function to all the examples in the table (individually or in batches) and update the table.\n",
      " |      If your function returns a column that already exists, then it overwrites it.\n",
      " |      \n",
      " |      You can specify whether the function should be batched or not with the `batched` parameter:\n",
      " |      \n",
      " |      - If batched is `False`, then the function takes 1 example in and should return 1 example.\n",
      " |        An example is a dictionary, e.g. `{\"text\": \"Hello there !\"}`.\n",
      " |      - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n",
      " |        A batch is a dictionary, e.g. a batch of 1 example is `{\"text\": [\"Hello there !\"]}`.\n",
      " |      - If batched is `True` and `batch_size` is `n > 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\n",
      " |        Note that the last batch may have less than `n` examples.\n",
      " |        A batch is a dictionary, e.g. a batch of `n` examples is `{\"text\": [\"Hello there !\"] * n}`.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`Callable`): Function with one of the following signatures:\n",
      " |      \n",
      " |              - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |              - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |      \n",
      " |              For advanced usage, the function can also return a `pyarrow.Table`.\n",
      " |              Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
      " |              If no function is provided, default to identity function: `lambda x: x`.\n",
      " |          with_indices (`bool`, defaults to `False`):\n",
      " |              Provide example indices to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
      " |          with_rank (`bool`, defaults to `False`):\n",
      " |              Provide process rank to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n",
      " |              The columns to be passed into `function`\n",
      " |              as positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`):\n",
      " |              Provide batch of examples to `function`.\n",
      " |          batch_size (`int`, *optional*, defaults to `1000`):\n",
      " |              Number of examples per batch provided to `function` if `batched=True`.\n",
      " |              If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n",
      " |          drop_last_batch (`bool`, defaults to `False`):\n",
      " |              Whether a last batch smaller than the batch_size should be\n",
      " |              dropped instead of being processed by the function.\n",
      " |          remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n",
      " |              Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (`str`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          features (`Optional[datasets.Features]`, defaults to `None`):\n",
      " |              Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `False`):\n",
      " |              Disallow null values in the table.\n",
      " |          fn_kwargs (`Dict`, *optional*, defaults to `None`):\n",
      " |              Keyword arguments to be passed to `function`.\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Max number of processes when generating cache. Already cached shards are loaded sequentially.\n",
      " |          suffix_template (`str`):\n",
      " |              If `cache_file_name` is specified, then this suffix\n",
      " |              will be added at the end of the base name of each. Defaults to `\"_{rank:05d}_of_{num_proc:05d}\"`. For example, if `cache_file_name` is \"processed.arrow\", then for\n",
      " |              `rank=1` and `num_proc=4`, the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix.\n",
      " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |          desc (`str`, *optional*, defaults to `None`):\n",
      " |              Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> def add_prefix(example):\n",
      " |      ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      " |      ...     return example\n",
      " |      >>> ds = ds.map(add_prefix)\n",
      " |      >>> ds[0:3][\"text\"]\n",
      " |      ['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n",
      " |       'Review: the soundtrack alone is worth the price of admission .',\n",
      " |       'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n",
      " |      \n",
      " |      # process a batch of examples\n",
      " |      >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      " |      # set number of processors\n",
      " |      >>> ds = ds.map(add_prefix, num_proc=4)\n",
      " |      ```\n",
      " |  \n",
      " |  prepare_for_task(self, task: Union[str, datasets.tasks.base.TaskTemplate], id: int = 0) -> 'Dataset'\n",
      " |      Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).\n",
      " |      \n",
      " |      Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.\n",
      " |      \n",
      " |      Args:\n",
      " |          task (`Union[str, TaskTemplate]`):\n",
      " |              The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:\n",
      " |      \n",
      " |              - `\"text-classification\"`\n",
      " |              - `\"question-answering\"`\n",
      " |      \n",
      " |              If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](./task_templates).\n",
      " |          id (`int`, defaults to `0`):\n",
      " |              The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, config_name: str = 'default', set_default: Optional[bool] = None, split: Optional[str] = None, data_dir: Optional[str] = None, commit_message: Optional[str] = None, commit_description: Optional[str] = None, private: Optional[bool] = False, token: Optional[str] = None, revision: Optional[str] = None, branch='deprecated', create_pr: Optional[bool] = False, max_shard_size: Union[str, int, NoneType] = None, num_shards: Optional[int] = None, embed_external_files: bool = True) -> huggingface_hub.hf_api.CommitInfo\n",
      " |      Pushes the dataset to the hub as a Parquet dataset.\n",
      " |      The dataset is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      " |      \n",
      " |      The resulting Parquet files are self-contained by default. If your dataset contains [`Image`] or [`Audio`]\n",
      " |      data, the Parquet files will store the bytes of your images or audio files.\n",
      " |      You can disable this by setting `embed_external_files` to `False`.\n",
      " |      \n",
      " |      Args:\n",
      " |          repo_id (`str`):\n",
      " |              The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n",
      " |              `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n",
      " |              of the logged-in user.\n",
      " |          config_name (`str`, defaults to \"default\"):\n",
      " |              The configuration name (or subset) of a dataset. Defaults to \"default\".\n",
      " |          set_default (`bool`, *optional*):\n",
      " |              Whether to set this configuration as the default one. Otherwise, the default configuration is the one\n",
      " |              named \"default\".\n",
      " |          split (`str`, *optional*):\n",
      " |              The name of the split that will be given to that dataset. Defaults to `self.split`.\n",
      " |          data_dir (`str`, *optional*):\n",
      " |              Directory name that will contain the uploaded data files. Defaults to the `config_name` if different\n",
      " |              from \"default\", else \"data\".\n",
      " |      \n",
      " |              <Added version=\"2.17.0\"/>\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload dataset\"`.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              Description of the commit that will be created.\n",
      " |              Additionally, description of the PR if a PR is created (`create_pr` is True).\n",
      " |      \n",
      " |              <Added version=\"2.16.0\"/>\n",
      " |          private (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the dataset repository should be set to private or not. Only affects repository creation:\n",
      " |              a repository that already exists will not be affected by that parameter.\n",
      " |          token (`str`, *optional*):\n",
      " |              An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
      " |              to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n",
      " |              if no token is passed and the user is not logged-in.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to. Defaults to the `\"main\"` branch.\n",
      " |      \n",
      " |              <Added version=\"2.15.0\"/>\n",
      " |          branch (`str`, *optional*):\n",
      " |              The git branch on which to push the dataset. This defaults to the default branch as specified\n",
      " |              in your repository, which defaults to `\"main\"`.\n",
      " |      \n",
      " |              <Deprecated version=\"2.15.0\">\n",
      " |      \n",
      " |              `branch` was deprecated in favor of `revision` in version 2.15.0 and will be removed in 3.0.0.\n",
      " |      \n",
      " |              </Deprecated>\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to create a PR with the uploaded files or directly commit.\n",
      " |      \n",
      " |              <Added version=\"2.15.0\"/>\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by\n",
      " |              a unit (like `\"5MB\"`).\n",
      " |          num_shards (`int`, *optional*):\n",
      " |              Number of shards to write. By default, the number of shards depends on `max_shard_size`.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          embed_external_files (`bool`, defaults to `True`):\n",
      " |              Whether to embed file bytes in the shards.\n",
      " |              In particular, this will do the following before the push for the fields of type:\n",
      " |      \n",
      " |              - [`Audio`] and [`Image`]: remove local path information and embed file content in the Parquet files.\n",
      " |      \n",
      " |      Return:\n",
      " |          huggingface_hub.CommitInfo\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\")\n",
      " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n",
      " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n",
      " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\", num_shards=1024)\n",
      " |      ```\n",
      " |      \n",
      " |      If your dataset has multiple splits (e.g. train/validation/test):\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> train_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"train\")\n",
      " |      >>> val_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"validation\")\n",
      " |      >>> # later\n",
      " |      >>> dataset = load_dataset(\"<organization>/<dataset_id>\")\n",
      " |      >>> train_dataset = dataset[\"train\"]\n",
      " |      >>> val_dataset = dataset[\"validation\"]\n",
      " |      ```\n",
      " |      \n",
      " |      If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n",
      " |      >>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n",
      " |      >>> # later\n",
      " |      >>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n",
      " |      >>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n",
      " |      ```\n",
      " |  \n",
      " |  remove_columns(self, column_names: Union[str, List[str]], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Remove one or several column(s) in the dataset and the features associated to them.\n",
      " |      \n",
      " |      You can also remove a column using [`~datasets.Dataset.map`] with `remove_columns` but the present method\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (`Union[str, List[str]]`):\n",
      " |              Name of the column(s) to remove.\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]: A copy of the dataset object without the columns to remove.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.remove_columns('label')\n",
      " |      Dataset({\n",
      " |          features: ['text'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      >>> ds.remove_columns(column_names=ds.column_names) # Removing all the columns returns an empty dataset with the `num_rows` property set to 0\n",
      " |      Dataset({\n",
      " |          features: [],\n",
      " |          num_rows: 0\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  rename_column(self, original_column_name: str, new_column_name: str, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Rename a column in the dataset, and move the features associated to the original column under the new column\n",
      " |      name.\n",
      " |      \n",
      " |      Args:\n",
      " |          original_column_name (`str`):\n",
      " |              Name of the column to rename.\n",
      " |          new_column_name (`str`):\n",
      " |              New name for the column.\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]: A copy of the dataset with a renamed column.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.rename_column('label', 'label_new')\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label_new'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  rename_columns(self, column_mapping: Dict[str, str], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Rename several columns in the dataset, and move the features associated to the original columns under\n",
      " |      the new column names.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_mapping (`Dict[str, str]`):\n",
      " |              A mapping of columns to rename to their new names\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]: A copy of the dataset with renamed columns\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
      " |      Dataset({\n",
      " |          features: ['text_new', 'label_new'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  reset_format(self)\n",
      " |      Reset `__getitem__` return format to python objects and all columns.\n",
      " |      \n",
      " |      Same as `self.set_format()`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      >>> ds.reset_format()\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      ```\n",
      " |  \n",
      " |  save_to_disk(self, dataset_path: Union[str, bytes, os.PathLike], fs='deprecated', max_shard_size: Union[str, int, NoneType] = None, num_shards: Optional[int] = None, num_proc: Optional[int] = None, storage_options: Optional[dict] = None)\n",
      " |      Saves a dataset to a dataset directory, or in a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n",
      " |      \n",
      " |      For [`Image`] and [`Audio`] data:\n",
      " |      \n",
      " |      All the Image() and Audio() data are stored in the arrow files.\n",
      " |      If you want to store paths or urls, please use the Value(\"string\") type.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_path (`str`):\n",
      " |              Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n",
      " |              of the dataset directory where the dataset will be saved to.\n",
      " |          fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n",
      " |              Instance of the remote filesystem where the dataset will be saved to.\n",
      " |      \n",
      " |              <Deprecated version=\"2.8.0\">\n",
      " |      \n",
      " |              `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n",
      " |              Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n",
      " |      \n",
      " |              </Deprecated>\n",
      " |      \n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
      " |              (like `\"50MB\"`).\n",
      " |          num_shards (`int`, *optional*):\n",
      " |              Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          num_proc (`int`, *optional*):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              Multiprocessing is disabled by default.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/directory\")\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/directory\", num_shards=1024)\n",
      " |      ```\n",
      " |  \n",
      " |  select(self, indices: Iterable, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create a new dataset with rows selected following the list/array of indices.\n",
      " |      \n",
      " |      Args:\n",
      " |          indices (`range`, `list`, `iterable`, `ndarray` or `Series`):\n",
      " |              Range, list or 1D-array of integer indices for indexing.\n",
      " |              If the indices correspond to a contiguous range, the Arrow table is simply sliced.\n",
      " |              However passing a list of indices that are not contiguous creates indices mapping, which is much less efficient,\n",
      " |              but still faster than recreating an Arrow table made of the requested rows.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the indices mapping in memory instead of writing it to a cache file.\n",
      " |          indices_cache_file_name (`str`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices mapping instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.select(range(4))\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 4\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  select_columns(self, column_names: Union[str, List[str]], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Select one or several column(s) in the dataset and the features\n",
      " |      associated to them.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (`Union[str, List[str]]`):\n",
      " |              Name of the column(s) to keep.\n",
      " |          new_fingerprint (`str`, *optional*):\n",
      " |              The new fingerprint of the dataset after transform. If `None`,\n",
      " |              the new fingerprint is computed using a hash of the previous\n",
      " |              fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]: A copy of the dataset object which only consists of\n",
      " |          selected columns.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.select_columns(['text'])\n",
      " |      Dataset({\n",
      " |          features: ['text'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  set_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
      " |      The format `type` (for example \"numpy\") is used to format batches when using `__getitem__`.\n",
      " |      It's also possible to use custom transforms for formatting using [`~datasets.Dataset.set_transform`].\n",
      " |      \n",
      " |      Args:\n",
      " |          type (`str`, *optional*):\n",
      " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n",
      " |              `None` means `__getitem__` returns python objects (default).\n",
      " |          columns (`List[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              `None` means `__getitem__` returns all columns (default).\n",
      " |          output_all_columns (`bool`, defaults to `False`):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |          **format_kwargs (additional keyword arguments):\n",
      " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |      \n",
      " |      It is possible to call [`~datasets.Dataset.map`] after calling `set_format`. Since `map` may add new columns, then the list of formatted columns\n",
      " |      gets updated. In this case, if you apply `map` on a dataset to add a new column, then this column will be formatted as:\n",
      " |      \n",
      " |          ```\n",
      " |          new formatted columns = (all columns - previously unformatted columns)\n",
      " |          ```\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type='numpy', columns=['text', 'label'])\n",
      " |      >>> ds.format\n",
      " |      {'type': 'numpy',\n",
      " |      'format_kwargs': {},\n",
      " |      'columns': ['text', 'label'],\n",
      " |      'output_all_columns': False}\n",
      " |      ```\n",
      " |  \n",
      " |  set_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
      " |      Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n",
      " |      As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n",
      " |      \n",
      " |      Args:\n",
      " |          transform (`Callable`, *optional*):\n",
      " |              User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n",
      " |              A formatting function is a callable that takes a batch (as a `dict`) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in `__getitem__`.\n",
      " |          columns (`List[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (`bool`, defaults to `False`):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      >>> def encode(batch):\n",
      " |      ...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
      " |      >>> ds.set_transform(encode)\n",
      " |      >>> ds[0]\n",
      " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1]),\n",
      " |       'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,\n",
      " |               20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,\n",
      " |               5637,  1998, 11690,  2336,  1012,   102]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |               0, 0])}\n",
      " |      ```\n",
      " |  \n",
      " |  shard(self, num_shards: int, index: int, contiguous: bool = False, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000) -> 'Dataset'\n",
      " |      Return the `index`-nth shard from dataset split into `num_shards` pieces.\n",
      " |      \n",
      " |      This shards deterministically. `dset.shard(n, i)` will contain all elements of dset whose\n",
      " |      index mod `n = i`.\n",
      " |      \n",
      " |      `dset.shard(n, i, contiguous=True)` will instead split dset into contiguous chunks,\n",
      " |      so it can be easily concatenated back together after processing. If `n % i == l`, then the\n",
      " |      first `l` shards will have length `(n // i) + 1`, and the remaining shards will have length `(n // i)`.\n",
      " |      `datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` will return\n",
      " |      a dataset with the same order as the original.\n",
      " |      \n",
      " |      Be sure to shard before using any randomizing operator (such as `shuffle`).\n",
      " |      It is best if the shard operator is used early in the dataset pipeline.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          num_shards (`int`):\n",
      " |              How many shards to split the dataset into.\n",
      " |          index (`int`):\n",
      " |              Which shard to select and return.\n",
      " |          contiguous: (`bool`, defaults to `False`):\n",
      " |              Whether to select contiguous blocks of indices for shards.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          indices_cache_file_name (`str`, *optional*):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices of each shard instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      >>> ds.shard(num_shards=2, index=0)\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 533\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  shuffle(self, seed: Optional[int] = None, generator: Optional[numpy.random._generator.Generator] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |      \n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |      \n",
      " |      Shuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to create an indices mapping.\n",
      " |      However as soon as your [`Dataset`] has an indices mapping, the speed can become 10x slower.\n",
      " |      This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren't reading contiguous chunks of data anymore.\n",
      " |      To restore the speed, you'd need to rewrite the entire dataset on your disk again using [`Dataset.flatten_indices`], which removes the indices mapping.\n",
      " |      This may take a lot of time depending of the size of your dataset though:\n",
      " |      \n",
      " |      ```python\n",
      " |      my_dataset[0]  # fast\n",
      " |      my_dataset = my_dataset.shuffle(seed=42)\n",
      " |      my_dataset[0]  # up to 10x slower\n",
      " |      my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
      " |      my_dataset[0]  # fast again\n",
      " |      ```\n",
      " |      \n",
      " |      In this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approximate shuffling method [`IterableDataset.shuffle`].\n",
      " |      It only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal:\n",
      " |      \n",
      " |      ```python\n",
      " |      my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=128)\n",
      " |      for example in enumerate(my_iterable_dataset):  # fast\n",
      " |          pass\n",
      " |      \n",
      " |      shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n",
      " |      \n",
      " |      for example in enumerate(shuffled_iterable_dataset):  # as fast as before\n",
      " |          pass\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          seed (`int`, *optional*):\n",
      " |              A seed to initialize the default BitGenerator if `generator=None`.\n",
      " |              If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |          generator (`numpy.random.Generator`, *optional*):\n",
      " |              Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n",
      " |          keep_in_memory (`bool`, default `False`):\n",
      " |              Keep the shuffled indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the shuffled indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (`str`, *optional*):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              shuffled indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      \n",
      " |      # set a seed\n",
      " |      >>> shuffled_ds = ds.shuffle(seed=42)\n",
      " |      >>> shuffled_ds['label'][:10]\n",
      " |      [1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  sort(self, column_names: Union[str, Sequence[str]], reverse: Union[bool, Sequence[bool]] = False, kind='deprecated', null_placement: str = 'at_end', keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create a new dataset sorted according to a single or multiple columns.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (`Union[str, Sequence[str]]`):\n",
      " |              Column name(s) to sort by.\n",
      " |          reverse (`Union[bool, Sequence[bool]]`, defaults to `False`):\n",
      " |              If `True`, sort by descending order rather than ascending. If a single bool is provided,\n",
      " |              the value is applied to the sorting of all column names. Otherwise a list of bools with the\n",
      " |              same length and order as column_names must be provided.\n",
      " |          kind (`str`, *optional*):\n",
      " |              Pandas algorithm for sorting selected in `{quicksort, mergesort, heapsort, stable}`,\n",
      " |              The default is `quicksort`. Note that both `stable` and `mergesort` use `timsort` under the covers and, in general,\n",
      " |              the actual implementation will vary with data type. The `mergesort` option is retained for backwards compatibility.\n",
      " |              <Deprecated version=\"2.8.0\">\n",
      " |      \n",
      " |              `kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.\n",
      " |      \n",
      " |              </Deprecated>\n",
      " |          null_placement (`str`, defaults to `at_end`):\n",
      " |              Put `None` values at the beginning if `at_start` or `first` or at the end if `at_end` or `last`\n",
      " |      \n",
      " |              <Added version=\"1.14.2\"/>\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the sorted indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the sorted indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (`str`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              sorted indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset('rotten_tomatoes', split='validation')\n",
      " |      >>> ds['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      >>> sorted_ds = ds.sort('label')\n",
      " |      >>> sorted_ds['label'][:10]\n",
      " |      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " |      >>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n",
      " |      >>> another_sorted_ds['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      ```\n",
      " |  \n",
      " |  to_csv(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, num_proc: Optional[int] = None, **to_csv_kwargs) -> int\n",
      " |      Exports the dataset to csv\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_buf (`PathLike` or `FileOrBuffer`):\n",
      " |              Either a path to a file or a BinaryIO.\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              Size of the batch to load in memory and write at once.\n",
      " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          num_proc (`int`, *optional*):\n",
      " |              Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing. `batch_size` in this case defaults to\n",
      " |              `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n",
      " |              value if you have sufficient compute power.\n",
      " |          **to_csv_kwargs (additional keyword arguments):\n",
      " |              Parameters to pass to pandas's [`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n",
      " |      \n",
      " |              <Changed version=\"2.10.0\">\n",
      " |      \n",
      " |              Now, `index` defaults to `False` if not specified.\n",
      " |      \n",
      " |              If you would like to write the index, pass `index=True` and also set a name for the index column by\n",
      " |              passing `index_label`.\n",
      " |      \n",
      " |              </Changed>\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of characters or bytes written.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_csv(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  to_dict(self, batch_size: Optional[int] = None, batched='deprecated') -> Union[dict, Iterator[dict]]\n",
      " |      Returns the dataset as a Python dict. Can also return a generator for large datasets.\n",
      " |      \n",
      " |      Args:\n",
      " |          batched (`bool`):\n",
      " |              Set to `True` to return a generator that yields the dataset as batches\n",
      " |              of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n",
      " |      \n",
      " |              <Deprecated version=\"2.11.0\">\n",
      " |      \n",
      " |              Use `.iter(batch_size=batch_size)` followed by `.to_dict()` on the individual batches instead.\n",
      " |      \n",
      " |              </Deprecated>\n",
      " |      \n",
      " |          batch_size (`int`, *optional*): The size (number of rows) of the batches if `batched` is `True`.\n",
      " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `dict` or `Iterator[dict]`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_dict()\n",
      " |      ```\n",
      " |  \n",
      " |  to_iterable_dataset(self, num_shards: Optional[int] = 1) -> 'IterableDataset'\n",
      " |      Get an [`datasets.IterableDataset`] from a map-style [`datasets.Dataset`].\n",
      " |      This is equivalent to loading a dataset in streaming mode with [`datasets.load_dataset`], but much faster since the data is streamed from local files.\n",
      " |      \n",
      " |      Contrary to map-style datasets, iterable datasets are lazy and can only be iterated over (e.g. using a for loop).\n",
      " |      Since they are read sequentially in training loops, iterable datasets are much faster than map-style datasets.\n",
      " |      All the transformations applied to iterable datasets like filtering or processing are done on-the-fly when you start iterating over the dataset.\n",
      " |      \n",
      " |      Still, it is possible to shuffle an iterable dataset using [`datasets.IterableDataset.shuffle`].\n",
      " |      This is a fast approximate shuffling that works best if you have multiple shards and if you specify a buffer size that is big enough.\n",
      " |      \n",
      " |      To get the best speed performance, make sure your dataset doesn't have an indices mapping.\n",
      " |      If this is the case, the data are not read contiguously, which can be slow sometimes.\n",
      " |      You can use `ds = ds.flatten_indices()` to write your dataset in contiguous chunks of data and have optimal speed before switching to an iterable dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |          num_shards (`int`, default to `1`):\n",
      " |              Number of shards to define when instantiating the iterable dataset. This is especially useful for big datasets to be able to shuffle properly,\n",
      " |              and also to enable fast parallel loading using a PyTorch DataLoader or in distributed setups for example.\n",
      " |              Shards are defined using [`datasets.Dataset.shard`]: it simply slices the data without writing anything on disk.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`datasets.IterableDataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      Basic usage:\n",
      " |      ```python\n",
      " |      >>> ids = ds.to_iterable_dataset()\n",
      " |      >>> for example in ids:\n",
      " |      ...     pass\n",
      " |      ```\n",
      " |      \n",
      " |      With lazy filtering and processing:\n",
      " |      ```python\n",
      " |      >>> ids = ds.to_iterable_dataset()\n",
      " |      >>> ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset\n",
      " |      >>> for example in ids:\n",
      " |      ...     pass\n",
      " |      ```\n",
      " |      \n",
      " |      With sharding to enable efficient shuffling:\n",
      " |      ```python\n",
      " |      >>> ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over\n",
      " |      >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating\n",
      " |      >>> for example in ids:\n",
      " |      ...     pass\n",
      " |      ```\n",
      " |      \n",
      " |      With a PyTorch DataLoader:\n",
      " |      ```python\n",
      " |      >>> import torch\n",
      " |      >>> ids = ds.to_iterable_dataset(num_shards=64)\n",
      " |      >>> ids = ids.filter(filter_fn).map(process_fn)\n",
      " |      >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating\n",
      " |      >>> for example in ids:\n",
      " |      ...     pass\n",
      " |      ```\n",
      " |      \n",
      " |      With a PyTorch DataLoader and shuffling:\n",
      " |      ```python\n",
      " |      >>> import torch\n",
      " |      >>> ids = ds.to_iterable_dataset(num_shards=64)\n",
      " |      >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
      " |      >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n",
      " |      >>> for example in ids:\n",
      " |      ...     pass\n",
      " |      ```\n",
      " |      \n",
      " |      In a distributed setup like PyTorch DDP with a PyTorch DataLoader and shuffling\n",
      " |      ```python\n",
      " |      >>> from datasets.distributed import split_dataset_by_node\n",
      " |      >>> ids = ds.to_iterable_dataset(num_shards=512)\n",
      " |      >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
      " |      >>> ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating\n",
      " |      >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating\n",
      " |      >>> for example in ids:\n",
      " |      ...     pass\n",
      " |      ```\n",
      " |      \n",
      " |      With shuffling and multiple epochs:\n",
      " |      ```python\n",
      " |      >>> ids = ds.to_iterable_dataset(num_shards=64)\n",
      " |      >>> ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
      " |      >>> for epoch in range(n_epochs):\n",
      " |      ...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating\n",
      " |      ...     for example in ids:\n",
      " |      ...         pass\n",
      " |      ```\n",
      " |      Feel free to also use [`IterableDataset.set_epoch`] when using a PyTorch DataLoader or in distributed setups.\n",
      " |  \n",
      " |  to_json(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, num_proc: Optional[int] = None, **to_json_kwargs) -> int\n",
      " |      Export the dataset to JSON Lines or JSON.\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_buf (`PathLike` or `FileOrBuffer`):\n",
      " |              Either a path to a file or a BinaryIO.\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              Size of the batch to load in memory and write at once.\n",
      " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          num_proc (`int`, *optional*):\n",
      " |              Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing. `batch_size` in this case defaults to\n",
      " |              `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n",
      " |              value if you have sufficient compute power.\n",
      " |          **to_json_kwargs (additional keyword arguments):\n",
      " |              Parameters to pass to pandas's [`pandas.DataFrame.to_json`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n",
      " |      \n",
      " |              <Changed version=\"2.11.0\">\n",
      " |      \n",
      " |              Now, `index` defaults to `False` if `orient` is `\"split\"` or `\"table\"`.\n",
      " |      \n",
      " |              If you would like to write the index, pass `index=True`.\n",
      " |      \n",
      " |              </Changed>\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of characters or bytes written.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_json(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  to_list(self) -> list\n",
      " |      Returns the dataset as a Python list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `list`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_list()\n",
      " |      ```\n",
      " |  \n",
      " |  to_pandas(self, batch_size: Optional[int] = None, batched: bool = False) -> Union[pandas.core.frame.DataFrame, Iterator[pandas.core.frame.DataFrame]]\n",
      " |      Returns the dataset as a `pandas.DataFrame`. Can also return a generator for large datasets.\n",
      " |      \n",
      " |      Args:\n",
      " |          batched (`bool`):\n",
      " |              Set to `True` to return a generator that yields the dataset as batches\n",
      " |              of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              The size (number of rows) of the batches if `batched` is `True`.\n",
      " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `pandas.DataFrame` or `Iterator[pandas.DataFrame]`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_pandas()\n",
      " |      ```\n",
      " |  \n",
      " |  to_parquet(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, **parquet_writer_kwargs) -> int\n",
      " |      Exports the dataset to parquet\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_buf (`PathLike` or `FileOrBuffer`):\n",
      " |              Either a path to a file or a BinaryIO.\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              Size of the batch to load in memory and write at once.\n",
      " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          **parquet_writer_kwargs (additional keyword arguments):\n",
      " |              Parameters to pass to PyArrow's `pyarrow.parquet.ParquetWriter`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of characters or bytes written.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_parquet(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  to_sql(self, name: str, con: Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')], batch_size: Optional[int] = None, **sql_writer_kwargs) -> int\n",
      " |      Exports the dataset to a SQL database.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (`str`):\n",
      " |              Name of SQL table.\n",
      " |          con (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`):\n",
      " |              A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) or a SQLite3/SQLAlchemy connection object used to write to a database.\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              Size of the batch to load in memory and write at once.\n",
      " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          **sql_writer_kwargs (additional keyword arguments):\n",
      " |              Parameters to pass to pandas's [`pandas.DataFrame.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).\n",
      " |      \n",
      " |              <Changed version=\"2.11.0\">\n",
      " |      \n",
      " |              Now, `index` defaults to `False` if not specified.\n",
      " |      \n",
      " |              If you would like to write the index, pass `index=True` and also set a name for the index column by\n",
      " |              passing `index_label`.\n",
      " |      \n",
      " |              </Changed>\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of records written.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> # con provided as a connection URI string\n",
      " |      >>> ds.to_sql(\"data\", \"sqlite:///my_own_db.sql\")\n",
      " |      >>> # con provided as a sqlite3 connection object\n",
      " |      >>> import sqlite3\n",
      " |      >>> con = sqlite3.connect(\"my_own_db.sql\")\n",
      " |      >>> with con:\n",
      " |      ...     ds.to_sql(\"data\", con)\n",
      " |      ```\n",
      " |  \n",
      " |  train_test_split(self, test_size: Union[float, int, NoneType] = None, train_size: Union[float, int, NoneType] = None, shuffle: bool = True, stratify_by_column: Optional[str] = None, seed: Optional[int] = None, generator: Optional[numpy.random._generator.Generator] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, train_indices_cache_file_name: Optional[str] = None, test_indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, train_new_fingerprint: Optional[str] = None, test_new_fingerprint: Optional[str] = None) -> 'DatasetDict'\n",
      " |      Return a dictionary ([`datasets.DatasetDict`]) with two random train and test subsets (`train` and `test` `Dataset` splits).\n",
      " |      Splits are created from the dataset according to `test_size`, `train_size` and `shuffle`.\n",
      " |      \n",
      " |      This method is similar to scikit-learn `train_test_split`.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_size (`numpy.random.Generator`, *optional*):\n",
      " |              Size of the test split\n",
      " |              If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the test split.\n",
      " |              If `int`, represents the absolute number of test samples.\n",
      " |              If `None`, the value is set to the complement of the train size.\n",
      " |              If `train_size` is also `None`, it will be set to `0.25`.\n",
      " |          train_size (`numpy.random.Generator`, *optional*):\n",
      " |              Size of the train split\n",
      " |              If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the train split.\n",
      " |              If `int`, represents the absolute number of train samples.\n",
      " |              If `None`, the value is automatically set to the complement of the test size.\n",
      " |          shuffle (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to shuffle the data before splitting.\n",
      " |          stratify_by_column (`str`, *optional*, defaults to `None`):\n",
      " |              The column name of labels to be used to perform stratified split of data.\n",
      " |          seed (`int`, *optional*):\n",
      " |              A seed to initialize the default BitGenerator if `generator=None`.\n",
      " |              If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |          generator (`numpy.random.Generator`, *optional*):\n",
      " |              Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the splits indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the splits indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          train_cache_file_name (`str`, *optional*):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              train split indices instead of the automatically generated cache file name.\n",
      " |          test_cache_file_name (`str`, *optional*):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              test split indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          train_new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the train set after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |          test_new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the test set after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 852\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 214\n",
      " |          })\n",
      " |      })\n",
      " |      \n",
      " |      # set a seed\n",
      " |      >>> ds = ds.train_test_split(test_size=0.2, seed=42)\n",
      " |      \n",
      " |      # stratified split\n",
      " |      >>> ds = load_dataset(\"imdb\",split=\"train\")\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 25000\n",
      " |      })\n",
      " |      >>> ds = ds.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 20000\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 5000\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  unique(self, column: str) -> List\n",
      " |      Return a list of the unique elements in a column.\n",
      " |      \n",
      " |      This is implemented in the low-level backend and as such, very fast.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              Column name (list all the column names with [`~datasets.Dataset.column_names`]).\n",
      " |      \n",
      " |      Returns:\n",
      " |          `list`: List of unique elements in the given column.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.unique('label')\n",
      " |      [1, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  with_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
      " |      The format `type` (for example \"numpy\") is used to format batches when using `__getitem__`.\n",
      " |      \n",
      " |      It's also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].\n",
      " |      \n",
      " |      Contrary to [`~datasets.Dataset.set_format`], `with_format` returns a new [`Dataset`] object.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (`str`, *optional*):\n",
      " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n",
      " |              `None` means `__getitem__` returns python objects (default).\n",
      " |          columns (`List[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              `None` means `__getitem__` returns all columns (default).\n",
      " |          output_all_columns (`bool`, defaults to `False`):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |          **format_kwargs (additional keyword arguments):\n",
      " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'tensorflow'}\n",
      " |      ```\n",
      " |  \n",
      " |  with_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
      " |      Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n",
      " |      \n",
      " |      As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n",
      " |      \n",
      " |      Contrary to [`~datasets.Dataset.set_transform`], `with_transform` returns a new [`Dataset`] object.\n",
      " |      \n",
      " |      Args:\n",
      " |          transform (`Callable`, `optional`):\n",
      " |              User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n",
      " |              A formatting function is a callable that takes a batch (as a `dict`) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in `__getitem__`.\n",
      " |          columns (`List[str]`, `optional`):\n",
      " |              Columns to format in the output.\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (`bool`, defaults to `False`):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |              If set to `True`, then the other un-formatted columns are kept with the output of the transform.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> def encode(example):\n",
      " |      ...     return tokenizer(example[\"text\"], padding=True, truncation=True, return_tensors='pt')\n",
      " |      >>> ds = ds.with_transform(encode)\n",
      " |      >>> ds[0]\n",
      " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1, 1, 1, 1]),\n",
      " |       'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n",
      " |               1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n",
      " |               1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |               0, 0, 0, 0, 0])}\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_buffer(buffer: pyarrow.lib.Buffer, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_buffer: Optional[pyarrow.lib.Buffer] = None) -> 'Dataset' from builtins.type\n",
      " |      Instantiate a Dataset backed by an Arrow buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |          buffer (`pyarrow.Buffer`):\n",
      " |              Arrow buffer.\n",
      " |          info (`DatasetInfo`, *optional*):\n",
      " |              Dataset information, like description, citation, etc.\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Name of the dataset split.\n",
      " |          indices_buffer (`pyarrow.Buffer`, *optional*):\n",
      " |              Indices Arrow buffer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |  \n",
      " |  from_dict(mapping: dict, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert `dict` to a `pyarrow.Table` to create a [`Dataset`].\n",
      " |      \n",
      " |      Args:\n",
      " |          mapping (`Mapping`):\n",
      " |              Mapping of strings to Arrays or Python lists.\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          info (`DatasetInfo`, *optional*):\n",
      " |              Dataset information, like description, citation, etc.\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Name of the dataset split.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |  \n",
      " |  from_file(filename: str, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_filename: Optional[str] = None, in_memory: bool = False) -> 'Dataset' from builtins.type\n",
      " |      Instantiate a Dataset backed by an Arrow table at filename.\n",
      " |      \n",
      " |      Args:\n",
      " |          filename (`str`):\n",
      " |              File name of the dataset.\n",
      " |          info (`DatasetInfo`, *optional*):\n",
      " |              Dataset information, like description, citation, etc.\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Name of the dataset split.\n",
      " |          indices_filename (`str`, *optional*):\n",
      " |              File names of the indices.\n",
      " |          in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |  \n",
      " |  from_list(mapping: List[dict], features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert a list of dicts to a `pyarrow.Table` to create a [`Dataset`]`.\n",
      " |      \n",
      " |      Note that the keys of the first entry will be used to determine the dataset columns,\n",
      " |      regardless of what is passed to features.\n",
      " |      \n",
      " |      Args:\n",
      " |          mapping (`List[dict]`): A list of mappings of strings to row values.\n",
      " |          features (`Features`, optional): Dataset features.\n",
      " |          info (`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      " |          split (`NamedSplit`, optional): Name of the dataset split.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |  \n",
      " |  from_pandas(df: pandas.core.frame.DataFrame, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, preserve_index: Optional[bool] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [`Dataset`].\n",
      " |      \n",
      " |      The column types in the resulting Arrow Table are inferred from the dtypes of the `pandas.Series` in the\n",
      " |      DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n",
      " |      case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n",
      " |      \n",
      " |      Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n",
      " |      type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n",
      " |      contains `None/nan` objects, the type is set to `null`. This behavior can be avoided by constructing explicit\n",
      " |      features and passing it to this function.\n",
      " |      \n",
      " |      Args:\n",
      " |          df (`pandas.DataFrame`):\n",
      " |              Dataframe that contains the dataset.\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          info (`DatasetInfo`, *optional*):\n",
      " |              Dataset information, like description, citation, etc.\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Name of the dataset split.\n",
      " |          preserve_index (`bool`, *optional*):\n",
      " |              Whether to store the index as an additional column in the resulting Dataset.\n",
      " |              The default of `None` will store the index as a column, except for `RangeIndex` which is stored as metadata only.\n",
      " |              Use `preserve_index=True` to force it to be stored as a column.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_pandas(df)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_csv(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, num_proc: Optional[int] = None, **kwargs)\n",
      " |      Create Dataset from CSV file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (`path-like` or list of `path-like`):\n",
      " |              Path(s) of the CSV file(s).\n",
      " |          split ([`NamedSplit`], *optional*):\n",
      " |              Split name to be assigned to the dataset.\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`pandas.read_csv`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_csv('path/to/dataset.csv')\n",
      " |      ```\n",
      " |  \n",
      " |  from_generator(generator: Callable, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, gen_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, **kwargs)\n",
      " |      Create a Dataset from a generator.\n",
      " |      \n",
      " |      Args:\n",
      " |          generator (:`Callable`):\n",
      " |              A generator function that `yields` examples.\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          gen_kwargs(`dict`, *optional*):\n",
      " |              Keyword arguments to be passed to the `generator` callable.\n",
      " |              You can define a sharded dataset by passing the list of shards in `gen_kwargs` and setting `num_proc` greater than 1.\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      " |              If `num_proc` is greater than one, then all list values in `gen_kwargs` must be the same length. These values will be split between calls to the generator. The number of shards will be the minimum of the shortest list in `gen_kwargs` and `num_proc`.\n",
      " |      \n",
      " |              <Added version=\"2.7.0\"/>\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to :[`GeneratorConfig`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> def gen():\n",
      " |      ...     yield {\"text\": \"Good\", \"label\": 0}\n",
      " |      ...     yield {\"text\": \"Bad\", \"label\": 1}\n",
      " |      ...\n",
      " |      >>> ds = Dataset.from_generator(gen)\n",
      " |      ```\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> def gen(shards):\n",
      " |      ...     for shard in shards:\n",
      " |      ...         with open(shard) as f:\n",
      " |      ...             for line in f:\n",
      " |      ...                 yield {\"line\": line}\n",
      " |      ...\n",
      " |      >>> shards = [f\"data{i}.txt\" for i in range(32)]\n",
      " |      >>> ds = Dataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n",
      " |      ```\n",
      " |  \n",
      " |  from_json(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, field: Optional[str] = None, num_proc: Optional[int] = None, **kwargs)\n",
      " |      Create Dataset from JSON or JSON Lines file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (`path-like` or list of `path-like`):\n",
      " |              Path(s) of the JSON or JSON Lines file(s).\n",
      " |          split ([`NamedSplit`], *optional*):\n",
      " |              Split name to be assigned to the dataset.\n",
      " |          features ([`Features`], *optional*):\n",
      " |               Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          field (`str`, *optional*):\n",
      " |              Field name of the JSON file where the dataset is contained in.\n",
      " |          num_proc (`int`, *optional* defaults to `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`JsonConfig`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_json('path/to/dataset.json')\n",
      " |      ```\n",
      " |  \n",
      " |  from_parquet(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, columns: Optional[List[str]] = None, num_proc: Optional[int] = None, **kwargs)\n",
      " |      Create Dataset from Parquet file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (`path-like` or list of `path-like`):\n",
      " |              Path(s) of the Parquet file(s).\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Split name to be assigned to the dataset.\n",
      " |          features (`Features`, *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          columns (`List[str]`, *optional*):\n",
      " |              If not `None`, only these columns will be read from the file.\n",
      " |              A column name may be a prefix of a nested field, e.g. 'a' will select\n",
      " |              'a.b', 'a.c', and 'a.d.e'.\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`ParquetConfig`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_parquet('path/to/dataset.parquet')\n",
      " |      ```\n",
      " |  \n",
      " |  from_spark(df: 'pyspark.sql.DataFrame', split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, keep_in_memory: bool = False, cache_dir: str = None, working_dir: str = None, load_from_cache_file: bool = True, **kwargs)\n",
      " |      Create a Dataset from Spark DataFrame. Dataset downloading is distributed over Spark workers.\n",
      " |      \n",
      " |      Args:\n",
      " |          df (`pyspark.sql.DataFrame`):\n",
      " |              The DataFrame containing the desired data.\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Split name to be assigned to the dataset.\n",
      " |          features (`Features`, *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data. When using a multi-node Spark cluster, the cache_dir must be accessible to both\n",
      " |              workers and the driver.\n",
      " |          keep_in_memory (`bool`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          working_dir (`str`, *optional*)\n",
      " |              Intermediate directory for each Spark worker to write data to before moving it to `cache_dir`. Setting\n",
      " |              a non-NFS intermediate directory may improve performance.\n",
      " |          load_from_cache_file (`bool`):\n",
      " |              Whether to load the dataset from the cache if possible.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      >>>     data=[[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]],\n",
      " |      >>>     columns=[\"id\", \"name\"],\n",
      " |      >>> )\n",
      " |      >>> ds = Dataset.from_spark(df)\n",
      " |      ```\n",
      " |  \n",
      " |  from_sql(sql: Union[str, ForwardRef('sqlalchemy.sql.Selectable')], con: Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs)\n",
      " |      Create Dataset from SQL query or database table.\n",
      " |      \n",
      " |      Args:\n",
      " |          sql (`str` or `sqlalchemy.sql.Selectable`):\n",
      " |              SQL query to be executed or a table name.\n",
      " |          con (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`):\n",
      " |              A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`SqlConfig`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> # Fetch a database table\n",
      " |      >>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n",
      " |      >>> # Execute a SQL query on the table\n",
      " |      >>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n",
      " |      >>> # Use a Selectable object to specify the query\n",
      " |      >>> from sqlalchemy import select, text\n",
      " |      >>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n",
      " |      >>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n",
      " |      ```\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      The returned dataset can only be cached if `con` is specified as URI string.\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  from_text(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, num_proc: Optional[int] = None, **kwargs)\n",
      " |      Create Dataset from text file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (`path-like` or list of `path-like`):\n",
      " |              Path(s) of the text file(s).\n",
      " |          split (`NamedSplit`, *optional*):\n",
      " |              Split name to be assigned to the dataset.\n",
      " |          features (`Features`, *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`TextConfig`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`]\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_text('path/to/dataset.txt')\n",
      " |      ```\n",
      " |  \n",
      " |  load_from_disk(dataset_path: str, fs='deprecated', keep_in_memory: Optional[bool] = None, storage_options: Optional[dict] = None) -> 'Dataset'\n",
      " |      Loads a dataset that was previously saved using [`save_to_disk`] from a dataset directory, or from a\n",
      " |      filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_path (`str`):\n",
      " |              Path (e.g. `\"dataset/train\"`) or remote URI (e.g. `\"s3//my-bucket/dataset/train\"`)\n",
      " |              of the dataset directory where the dataset will be loaded from.\n",
      " |          fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n",
      " |              Instance of the remote filesystem where the dataset will be saved to.\n",
      " |      \n",
      " |              <Deprecated version=\"2.8.0\">\n",
      " |      \n",
      " |              `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n",
      " |              Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n",
      " |      \n",
      " |              </Deprecated>\n",
      " |      \n",
      " |          keep_in_memory (`bool`, defaults to `None`):\n",
      " |              Whether to copy the dataset in-memory. If `None`, the\n",
      " |              dataset will not be copied in-memory unless explicitly enabled by setting\n",
      " |              `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n",
      " |              [improve performance](../cache#improve-performance) section.\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |      \n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`Dataset`] or [`DatasetDict`]:\n",
      " |          - If `dataset_path` is a path of a dataset directory, the dataset requested.\n",
      " |          - If `dataset_path` is a path of a dataset dict directory, a `datasets.DatasetDict` with each split.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = load_from_disk(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  cache_files\n",
      " |      The cache files containing the Apache Arrow table backing the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.cache_files\n",
      " |      [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]\n",
      " |      ```\n",
      " |  \n",
      " |  column_names\n",
      " |      Names of the columns in the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.column_names\n",
      " |      ['text', 'label']\n",
      " |      ```\n",
      " |  \n",
      " |  data\n",
      " |      The Apache Arrow table backing the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.data\n",
      " |      MemoryMappedTable\n",
      " |      text: string\n",
      " |      label: int64\n",
      " |      ----\n",
      " |      text: [[\"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\"the soundtrack alone is worth the price of admission .\",\"rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\",\"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\",\"bielinsky is a filmmaker of impressive talent .\",\"so beautifully acted and directed , it's clear that washington most certainly has a new career ahead of him if he so chooses .\",\"a visual spectacle full of stunning images and effects .\",\"a gentle and engrossing character study .\",\"it's enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins .\",\"an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line .\",...,\"ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with .\",\"ah-nuld's action hero days might be over .\",\"it's clear why deuces wild , which was shot two years ago , has been gathering dust on mgm's shelf .\",\"feels like nothing quite so much as a middle-aged moviemaker's attempt to surround himself with beautiful , half-naked women .\",\"when the precise nature of matthew's predicament finally comes into sharp focus , the revelation fails to justify the build-up .\",\"this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .\",\"hilarious musical comedy though stymied by accents thick as mud .\",\"if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .\",\"a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .\",\"the feature-length stretch . . . strains the show's concept .\"]]\n",
      " |      label: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]\n",
      " |      ```\n",
      " |  \n",
      " |  features\n",
      " |  \n",
      " |  format\n",
      " |  \n",
      " |  num_columns\n",
      " |      Number of columns in the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.num_columns\n",
      " |      2\n",
      " |      ```\n",
      " |  \n",
      " |  num_rows\n",
      " |      Number of rows in the dataset (same as [`Dataset.__len__`]).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.num_rows\n",
      " |      1066\n",
      " |      ```\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of the dataset (number of columns, number of rows).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.shape\n",
      " |      (1066, 2)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from DatasetInfoMixin:\n",
      " |  \n",
      " |  builder_name\n",
      " |  \n",
      " |  citation\n",
      " |  \n",
      " |  config_name\n",
      " |  \n",
      " |  dataset_size\n",
      " |  \n",
      " |  description\n",
      " |  \n",
      " |  download_checksums\n",
      " |  \n",
      " |  download_size\n",
      " |  \n",
      " |  homepage\n",
      " |  \n",
      " |  info\n",
      " |      [`~datasets.DatasetInfo`] object containing all the metadata in the dataset.\n",
      " |  \n",
      " |  license\n",
      " |  \n",
      " |  size_in_bytes\n",
      " |  \n",
      " |  split\n",
      " |      [`~datasets.NamedSplit`] object corresponding to a named dataset split.\n",
      " |  \n",
      " |  supervised_keys\n",
      " |  \n",
      " |  task_templates\n",
      " |  \n",
      " |  version\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetInfoMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from datasets.search.IndexableMixin:\n",
      " |  \n",
      " |  drop_index(self, index_name: str)\n",
      " |      Drop the index with the specified column.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`):\n",
      " |              The `index_name`/identifier of the index.\n",
      " |  \n",
      " |  get_index(self, index_name: str) -> datasets.search.BaseIndex\n",
      " |      List the `index_name`/identifiers of all the attached indexes.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`): Index name.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`BaseIndex`]\n",
      " |  \n",
      " |  get_nearest_examples(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10, **kwargs) -> datasets.search.NearestExamplesResults\n",
      " |      Find the nearest examples in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |          query (`Union[str, np.ndarray]`):\n",
      " |              The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (`int`):\n",
      " |              The number of examples to retrieve.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `(scores, examples)`:\n",
      " |              A tuple of `(scores, examples)` where:\n",
      " |              - **scores** (`List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples\n",
      " |              - **examples** (`dict`): the retrieved examples\n",
      " |  \n",
      " |  get_nearest_examples_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10, **kwargs) -> datasets.search.BatchedNearestExamplesResults\n",
      " |      Find the nearest examples in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`):\n",
      " |              The `index_name`/identifier of the index.\n",
      " |          queries (`Union[List[str], np.ndarray]`):\n",
      " |              The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (`int`):\n",
      " |              The number of examples to retrieve per query.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `(total_scores, total_examples)`:\n",
      " |              A tuple of `(total_scores, total_examples)` where:\n",
      " |              - **total_scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query\n",
      " |              - **total_examples** (`List[dict]`): the retrieved examples per query\n",
      " |  \n",
      " |  is_index_initialized(self, index_name: str) -> bool\n",
      " |  \n",
      " |  list_indexes(self) -> List[str]\n",
      " |      List the `colindex_nameumns`/identifiers of all the attached indexes.\n",
      " |  \n",
      " |  load_elasticsearch_index(self, index_name: str, es_index_name: str, host: Optional[str] = None, port: Optional[int] = None, es_client: Optional[ForwardRef('Elasticsearch')] = None, es_index_config: Optional[dict] = None)\n",
      " |      Load an existing text index using ElasticSearch for fast retrieval.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`):\n",
      " |              The `index_name`/identifier of the index. This is the index name that is used to call `get_nearest` or `search`.\n",
      " |          es_index_name (`str`):\n",
      " |              The name of elasticsearch index to load.\n",
      " |          host (`str`, *optional*, defaults to `localhost`):\n",
      " |              Host of where ElasticSearch is running.\n",
      " |          port (`str`, *optional*, defaults to `9200`):\n",
      " |              Port of where ElasticSearch is running.\n",
      " |          es_client (`elasticsearch.Elasticsearch`, *optional*):\n",
      " |              The elasticsearch client used to create the index if host and port are `None`.\n",
      " |          es_index_config (`dict`, *optional*):\n",
      " |              The configuration of the elasticsearch index.\n",
      " |              Default config is:\n",
      " |                  ```\n",
      " |                  {\n",
      " |                      \"settings\": {\n",
      " |                          \"number_of_shards\": 1,\n",
      " |                          \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
      " |                      },\n",
      " |                      \"mappings\": {\n",
      " |                          \"properties\": {\n",
      " |                              \"text\": {\n",
      " |                                  \"type\": \"text\",\n",
      " |                                  \"analyzer\": \"standard\",\n",
      " |                                  \"similarity\": \"BM25\"\n",
      " |                              },\n",
      " |                          }\n",
      " |                      },\n",
      " |                  }\n",
      " |                  ```\n",
      " |  \n",
      " |  load_faiss_index(self, index_name: str, file: Union[str, pathlib.PurePath], device: Union[int, List[int], NoneType] = None, storage_options: Optional[Dict] = None)\n",
      " |      Load a FaissIndex from disk.\n",
      " |      \n",
      " |      If you want to do additional configurations, you can have access to the faiss index object by doing\n",
      " |      `.get_index(index_name).faiss_index` to make it fit your needs.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to\n",
      " |              call `.get_nearest` or `.search`.\n",
      " |          file (`str`): The path to the serialized faiss index on disk or remote URI (e.g. `\"s3://my-bucket/index.faiss\"`).\n",
      " |          device (Optional `Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
      " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |      \n",
      " |              <Added version=\"2.11.0\"/>\n",
      " |  \n",
      " |  save_faiss_index(self, index_name: str, file: Union[str, pathlib.PurePath], storage_options: Optional[Dict] = None)\n",
      " |      Save a FaissIndex on disk.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n",
      " |          file (`str`): The path to the serialized faiss index on disk or remote URI (e.g. `\"s3://my-bucket/index.faiss\"`).\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |      \n",
      " |              <Added version=\"2.11.0\"/>\n",
      " |  \n",
      " |  search(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10, **kwargs) -> datasets.search.SearchResults\n",
      " |      Find the nearest examples indices in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`):\n",
      " |              The name/identifier of the index.\n",
      " |          query (`Union[str, np.ndarray]`):\n",
      " |              The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (`int`):\n",
      " |              The number of examples to retrieve.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `(scores, indices)`:\n",
      " |              A tuple of `(scores, indices)` where:\n",
      " |              - **scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples\n",
      " |              - **indices** (`List[List[int]]`): the indices of the retrieved examples\n",
      " |  \n",
      " |  search_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10, **kwargs) -> datasets.search.BatchedSearchResults\n",
      " |      Find the nearest examples indices in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (`str`):\n",
      " |              The `index_name`/identifier of the index.\n",
      " |          queries (`Union[List[str], np.ndarray]`):\n",
      " |              The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (`int`):\n",
      " |              The number of examples to retrieve per query.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `(total_scores, total_indices)`:\n",
      " |              A tuple of `(total_scores, total_indices)` where:\n",
      " |              - **total_scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query\n",
      " |              - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from TensorflowDatasetMixin:\n",
      " |  \n",
      " |  to_tf_dataset(self, batch_size: Optional[int] = None, columns: Union[str, List[str], NoneType] = None, shuffle: bool = False, collate_fn: Optional[Callable] = None, drop_remainder: bool = False, collate_fn_args: Optional[Dict[str, Any]] = None, label_cols: Union[str, List[str], NoneType] = None, prefetch: bool = True, num_workers: int = 0, num_test_batches: int = 20)\n",
      " |      Create a `tf.data.Dataset` from the underlying Dataset. This `tf.data.Dataset` will load and collate batches from\n",
      " |      the Dataset, and is suitable for passing to methods like `model.fit()` or `model.predict()`. The dataset will yield\n",
      " |      `dicts` for both inputs and labels unless the `dict` would contain only a single key, in which case a raw\n",
      " |      `tf.Tensor` is yielded instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size (`int`, *optional*):\n",
      " |              Size of batches to load from the dataset. Defaults to `None`, which implies that the dataset won't be\n",
      " |              batched, but the returned dataset can be batched later with `tf_dataset.batch(batch_size)`.\n",
      " |          columns (`List[str]` or `str`, *optional*):\n",
      " |              Dataset column(s) to load in the `tf.data.Dataset`.\n",
      " |              Column names that are created by the `collate_fn` and that do not exist in the original dataset can be used.\n",
      " |          shuffle(`bool`, defaults to `False`):\n",
      " |              Shuffle the dataset order when loading. Recommended `True` for training, `False` for\n",
      " |              validation/evaluation.\n",
      " |          drop_remainder(`bool`, defaults to `False`):\n",
      " |              Drop the last incomplete batch when loading. Ensures\n",
      " |              that all batches yielded by the dataset will have the same length on the batch dimension.\n",
      " |          collate_fn(`Callable`, *optional*):\n",
      " |              A function or callable object (such as a `DataCollator`) that will collate\n",
      " |              lists of samples into a batch.\n",
      " |          collate_fn_args (`Dict`, *optional*):\n",
      " |              An optional `dict` of keyword arguments to be passed to the\n",
      " |              `collate_fn`.\n",
      " |          label_cols (`List[str]` or `str`, defaults to `None`):\n",
      " |              Dataset column(s) to load as labels.\n",
      " |              Note that many models compute loss internally rather than letting Keras do it, in which case\n",
      " |              passing the labels here is optional, as long as they're in the input `columns`.\n",
      " |          prefetch (`bool`, defaults to `True`):\n",
      " |              Whether to run the dataloader in a separate thread and maintain\n",
      " |              a small buffer of batches for training. Improves performance by allowing data to be loaded in the\n",
      " |              background while the model is training.\n",
      " |          num_workers (`int`, defaults to `0`):\n",
      " |              Number of workers to use for loading the dataset. Only supported on Python versions >= 3.8.\n",
      " |          num_test_batches (`int`, defaults to `20`):\n",
      " |              Number of batches to use to infer the output signature of the dataset.\n",
      " |              The higher this number, the more accurate the signature will be, but the longer it will take to\n",
      " |              create the dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `tf.data.Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds_train = ds[\"train\"].to_tf_dataset(\n",
      " |      ...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |      ...    shuffle=True,\n",
      " |      ...    batch_size=16,\n",
      " |      ...    collate_fn=data_collator,\n",
      " |      ... )\n",
      " |      ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(d['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = load_dataset('Ade_corpus_v2_drug_ade_relation')\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glkb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
